{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOL+bD2of/0+CPqBGfsB4xh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install schedule\n",
        "!pip install pytz             #for timezones"
      ],
      "metadata": {
        "id": "BUMLc8ewWZoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import re\n",
        "from dateutil.parser import parse\n",
        "import csv\n",
        "import schedule\n",
        "import time\n",
        "from pytz import all_timezones\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "wPYcJUOW6LVY"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the root url for scraping multiple pages\n",
        "dse_url = 'https://dsebd.org'\n",
        "listing_url = f'{dse_url}/company_listing.php'"
      ],
      "metadata": {
        "id": "yaYsI8TTNaA1"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trading_code(all_links):\n",
        "  links = []\n",
        "  trading_codes = []\n",
        "\n",
        "  # Get trading code\n",
        "  for link in all_links:\n",
        "    if 'name' in link:\n",
        "      trading_code = link.split('=')[-1]\n",
        "      trading_codes.append(trading_code)\n",
        "      links.append(link)\n",
        "\n",
        "  return trading_codes, links"
      ],
      "metadata": {
        "id": "uif6DDEr9tNB"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_in_csv(trading_codes, company_rows, percentage_rows):\n",
        "  with open('companies.csv', 'w', newline='') as company_file:\n",
        "    writer = csv.writer(company_file)\n",
        "    field = [\"Name\", \"Scrip Code\", \"Sector\", \"URL\"]\n",
        "\n",
        "    writer.writerow(field)\n",
        "    writer.writerows(company_rows)\n",
        "\n",
        "  with open('share_holding_percentage.csv', 'w', newline='') as share_file:\n",
        "    writer = csv.writer(share_file)\n",
        "    columns = [\"Date\", \"Sponsor/Director\", \"Govt\", \"Institute\", \"Foreign\", \"Public\"]\n",
        "\n",
        "    writer.writerow(columns)\n",
        "    writer.writerows(percentage_rows)\n",
        "\n",
        "  df = pd.read_csv('companies.csv')\n",
        "  df['trading_code'] = pd.Series(trading_codes)\n",
        "  df.to_csv('companies.csv')"
      ],
      "metadata": {
        "id": "fHUPxRmwP0d6"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dsebd_scrape():\n",
        "  # Get webpage content\n",
        "  company_list_source = requests.get(listing_url)\n",
        "  soup_listing = bs(company_list_source.content, 'lxml')\n",
        "\n",
        "  company_list = soup_listing.find('div', class_ = 'row al-li')                                                       #list of companies\n",
        "\n",
        "  all_links = [link['href'] for link in company_list.find_all('a', href = True)]                                      #links of companies only\n",
        "\n",
        "  trading_codes, links = get_trading_code(all_links)                                                                  #get trading codes\n",
        "\n",
        "  # Get all rows for 2 csv files\n",
        "  company_rows = []                                                                                                   #empty lists to store all the rows\n",
        "  percentage_rows = []\n",
        "\n",
        "  for link in links:\n",
        "    company_row = []\n",
        "    try:\n",
        "      url = f'{dse_url}/{link}'                                                                                       #go to url of each company\n",
        "      print(url)\n",
        "      company_page = requests.get(url)\n",
        "      soup_company = bs(company_page.text, 'lxml')\n",
        "\n",
        "      name_box = soup_company.find('h2', class_ = 'BodyHead topBodyHead')                                             #get company name\n",
        "      company_name = name_box.i.text\n",
        "      company_row.append(company_name)\n",
        "      print(company_name)\n",
        "\n",
        "      codes_table = soup_company.find('table', class_ = 'table table-bordered background-white shares-table')         #get scrip code\n",
        "\n",
        "      if 'Company' not in name_box.text:\n",
        "        codes_table = codes_table.find('tr', class_ = 'alt')\n",
        "\n",
        "      scrip_code = (codes_table.text).split()[-1]\n",
        "      company_row.append(scrip_code)\n",
        "\n",
        "      sector = (soup_company.find('th', string = 'Sector').find_next_sibling()).text                                  #get sector\n",
        "      company_row.append(sector)\n",
        "\n",
        "      company_row.append(url)\n",
        "\n",
        "    except(AttributeError, KeyError):\n",
        "      continue\n",
        "\n",
        "    company_rows.append(company_row)\n",
        "\n",
        "    share_holding_info = soup_company.find_all('td', string = re.compile('Share Holding Percentage'))                 #find share holding percentage for each category at a given date\n",
        "\n",
        "    for info in share_holding_info:\n",
        "\n",
        "      percentage_row = []\n",
        "\n",
        "      try:\n",
        "        date_time = parse(info.text, fuzzy = True)                                                                    #get date\n",
        "      except Exception:\n",
        "        continue\n",
        "\n",
        "      share_percentage_string = ((info.find_next_sibling()).text).split()                                             #get individual percentage\n",
        "      share_percentage = []\n",
        "      for percentage_value in share_percentage_string:\n",
        "\n",
        "        try:\n",
        "          share_percentage.append(float(percentage_value))\n",
        "\n",
        "        except(ValueError):\n",
        "          continue\n",
        "\n",
        "      percentage_row = [date_time.date()] + share_percentage\n",
        "      percentage_rows.append(percentage_row)\n",
        "\n",
        "  write_in_csv(trading_codes, company_rows, percentage_rows)"
      ],
      "metadata": {
        "id": "Yw8FtLj_YL6x"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  schedule.every().day.at(\"17:00\", \"Asia/Dhaka\").do(dsebd_scrape)\n",
        "\n",
        "  while(True):\n",
        "    schedule.run_pending()\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "t_KRWNH89zOx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}