{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMpNcPrjymDJ5cawhptJkV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install schedule\n",
        "!pip install pytz             #for timezones"
      ],
      "metadata": {
        "id": "BUMLc8ewWZoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import re\n",
        "from dateutil.parser import parse\n",
        "import csv\n",
        "import schedule\n",
        "import time\n",
        "from pytz import all_timezones\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "wPYcJUOW6LVY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the root url for scraping multiple pages\n",
        "dse_url = 'https://dsebd.org'\n",
        "listing_url = f'{dse_url}/company_listing.php'"
      ],
      "metadata": {
        "id": "yaYsI8TTNaA1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to get links of the company pages only\n",
        "def get_links(all_links):\n",
        "  links = []\n",
        "\n",
        "  for link in all_links:\n",
        "    if 'name' in link:\n",
        "      links.append(link)\n",
        "\n",
        "  return links"
      ],
      "metadata": {
        "id": "uif6DDEr9tNB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to write data to 2 csv files\n",
        "def write_in_csv(company_rows, percentage_rows):\n",
        "  with open('companies.csv', 'w', newline='') as company_file:\n",
        "    writer = csv.writer(company_file)\n",
        "    field = [\"Trading Code\", \"Name\", \"Scrip Code\", \"Sector\", \"URL\"]\n",
        "\n",
        "    writer.writerow(field)\n",
        "    writer.writerows(company_rows)\n",
        "\n",
        "  with open('share_holding_percentage.csv', 'w', newline='') as share_file:\n",
        "    writer = csv.writer(share_file)\n",
        "    columns = [\"Trading Code\", \"Date\", \"Sponsor/Director\", \"Govt\", \"Institute\", \"Foreign\", \"Public\"]\n",
        "\n",
        "    writer.writerow(columns)\n",
        "    writer.writerows(percentage_rows)"
      ],
      "metadata": {
        "id": "fHUPxRmwP0d6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method where the whole program works\n",
        "def dsebd_scrape():\n",
        "  # Get content of the webpage where the companies' links are listed\n",
        "  company_list_source = requests.get(listing_url)\n",
        "  soup_listing = bs(company_list_source.content, 'lxml')\n",
        "\n",
        "  company_list = soup_listing.find('div', class_ = 'row al-li')                                                       #list of companies\n",
        "\n",
        "  all_links = [link['href'] for link in company_list.find_all('a', href = True)]                                      #links of companies only\n",
        "\n",
        "  links = get_links(all_links)                                                                                        #get links from method\n",
        "\n",
        "  # Get all rows for 2 csv files\n",
        "  company_rows = []                                                                                                   #empty lists to store all the rows\n",
        "  percentage_rows = []\n",
        "\n",
        "  for link in links:\n",
        "    company_row = []\n",
        "    try:                                                                                                              #handle exception to prevent NaN values\n",
        "      url = f'{dse_url}/{link}'                                                                                       #go to url of each company\n",
        "      company_page = requests.get(url)\n",
        "      soup_company = bs(company_page.content, 'lxml')\n",
        "\n",
        "      name_box = soup_company.find('h2', class_ = 'BodyHead topBodyHead')                                             #get company name\n",
        "\n",
        "      if name_box is None:\n",
        "        continue\n",
        "\n",
        "      trading_code = link.split('=')[-1]                                                                              #get trading code\n",
        "      company_row.append(trading_code)\n",
        "\n",
        "      company_name = name_box.i.text\n",
        "      company_row.append(company_name)\n",
        "      print(company_name)\n",
        "\n",
        "      codes_table = soup_company.find('table', class_ = 'table table-bordered background-white shares-table')         #get scrip code\n",
        "\n",
        "      if 'Company' not in name_box.text:\n",
        "        codes_table = codes_table.find('tr', class_ = 'alt')\n",
        "\n",
        "      scrip_code = (codes_table.text).split()[-1]\n",
        "      company_row.append(scrip_code)\n",
        "\n",
        "      sector = (soup_company.find('th', string = 'Sector').find_next_sibling()).text                                  #get sector\n",
        "      company_row.append(sector)\n",
        "\n",
        "      company_row.append(url)\n",
        "\n",
        "    except(AttributeError, KeyError):\n",
        "      continue\n",
        "\n",
        "    company_rows.append(company_row)                                                                                  #single row of 'companies' csv file\n",
        "\n",
        "    share_holding_info = soup_company.find_all('td', string = re.compile('Share Holding Percentage'))                 #find share holding percentage for each category at a given date\n",
        "\n",
        "    for info in share_holding_info:\n",
        "\n",
        "      percentage_row = []\n",
        "      percentage_row.append(trading_code)\n",
        "\n",
        "      try:                                                                                                            #handle exception to extract the dates only\n",
        "        date_time = parse(info.text, fuzzy = True)                                                                    #get date using parser\n",
        "      except Exception:\n",
        "        continue\n",
        "\n",
        "      percentage_row.append(date_time.date())\n",
        "      share_percentage_values = ((info.find_next_sibling()).text).split()                                             #get percentage values along with the categories\n",
        "\n",
        "      for percentage_value in share_percentage_values:\n",
        "\n",
        "        try:                                                                                                          #handle exception to append the percentage values only\n",
        "          percentage_row.append(float(percentage_value))\n",
        "\n",
        "        except(ValueError):\n",
        "          continue\n",
        "\n",
        "\n",
        "      percentage_rows.append(percentage_row)                                                                          #single row of 'sharing_holding_percentage' csv file\n",
        "\n",
        "  write_in_csv(company_rows, percentage_rows)                                                                         #method to write in csv files"
      ],
      "metadata": {
        "id": "Yw8FtLj_YL6x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  schedule.every().day.at(\"17:00\", \"Asia/Dhaka\").do(dsebd_scrape)\n",
        "\n",
        "  while(True):\n",
        "    schedule.run_pending()\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "t_KRWNH89zOx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}